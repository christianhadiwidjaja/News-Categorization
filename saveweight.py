# -*- coding: utf-8 -*-
"""tesSaveweight.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18_D5n-dZL1N0eV-tdWgGQwqdiNub-WeF
"""

## for data
import json
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
from collections import Counter
import re
from sklearn import model_selection,metrics
from tensorflow.keras import models, layers, preprocessing as kprocessing
from tensorflow.keras import backend as K## for bert language model
import tensorflow
import transformers
import shutil

tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

## inputs
idx = layers.Input((50), dtype="int32", name="input_idx")
masks = layers.Input((50), dtype="int32", name="input_masks")
segments = layers.Input((50), dtype="int32", name="input_segments")## pre-trained bert
nlp = transformers.TFBertModel.from_pretrained("bert-base-uncased")
bert_out = nlp(idx, masks, segments)## fine-tuning

x = layers.GlobalAveragePooling1D()(bert_out[0])
x = layers.Dense(64, activation="relu")(x)
x = layers.Dense(128, activation="relu")(x)
x = layers.Dense(64, activation="relu")(x)
y_out = layers.Dense(7, activation='softmax')(x)## compile
model = models.Model([idx, masks, segments], y_out)

for layer in model.layers[:4]:
    layer.trainable = False
model.compile(loss='sparse_categorical_crossentropy', 
              optimizer='adam', metrics=['accuracy'])

status = model.load_weights(filepath='bert-classification.h5')
with open('dicy.json', 'r') as fp:
    dic_y_mapping = json.load(fp)



def cpredict(raw_text):
  corpus = raw_text
  corpus = np.array([str(corpus)])
  maxlen = 50
  ## add special tokens
  maxqnans = np.int((maxlen-20)/2)
  corpus_tokenized = ["[CLS] "+
              " ".join(tokenizer.tokenize(re.sub(r'[^\w\s]+|\n', '', 
              str(txt).lower().strip()))[:maxqnans])+
              " [SEP] " for txt in corpus]

  ## generate masks
  masks = [[1]*len(txt.split(" ")) + [0]*(maxlen - len(
            txt.split(" "))) for txt in corpus_tokenized]
  ## padding
  txt2seq = [txt + " [PAD]"*(maxlen-len(txt.split(" "))) if len(txt.split(" ")) != maxlen else txt for txt in corpus_tokenized]

  ## generate idx
  idx = [tokenizer.encode(seq.split(" "), max_length=50, truncation=True) for seq in txt2seq]
  print(corpus_tokenized)
  ## generate segments
  segments = [] 
  for seq in txt2seq:
      temp, i = [], 0
      for token in seq.split(" "):
          temp.append(i)
          if token == "[SEP]":
              i += 1
      segments.append(temp)## feature matrix
  text = [np.asarray(idx, dtype='int32'), 
            np.asarray(masks, dtype='int32'), 
            np.asarray(segments, dtype='int32')]

  predicted_prob = model.predict(text)
  predicted = [dic_y_mapping[str(np.argmax(pred))] for pred in 
             predicted_prob]
  print(predicted[0])

text="TOEFL ‘Essentials’: A Shorter, Less Costly English Test"
cpredict(text)